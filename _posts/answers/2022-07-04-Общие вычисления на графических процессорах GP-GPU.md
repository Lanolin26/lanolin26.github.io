---
layout: post
title: "Общие вычисления на графических процессорах GP-GPU"
categories: sppo-questions
---

Два процессора можно сравнить по разным критериям, наверное, самые популярные — это частота и количество ядер, размер
кэшей и прочее, но в конечном счете, нас интересует, сколько операций процессор может выполнить за единицу времени, что
это за операции вопрос отдельный, но наиболее распространенной метрикой является количество операций с плавающей запятой
в секунду — flops. И когда мы хотим сравнить теплое с мягким, а в нашем случае GPU с CPU, эта метрика приходится как
нельзя кстати.

## Архитектура GPU и ее сравнение с CPU

Привожу многим знакомую картинку с архитектурой CPU и основными элементами:

![CPU](/assets/q31-1.png)

А теперь давайте посмотрим на архитектуру GPU:

![GPU](/assets/q31-2.png)

У видеокарты множество вычислительных ядер, обычно несколько тысяч, но они объединены в блоки, для видеокарт NVIDIA
обычно по 32, и имеют общие элементы, в т.ч. и регистры. Архитектура ядра GPU и логических элементов существенно проще,
чем на CPU, а именно, нет префетчеров, бранч-предикторов и много чего еще.

Что же, это ключевые моменты отличия в архитектуре CPU и GPU, и, собственно, они и накладывают ограничения или,
наоборот, открывают возможности к тому, что мы можем эффективно считать на GPU.

Я не упомянул еще один важный момент, обычно, видеокарта и процессор не «шарят» память между собой и записать данные на
видеокарту и считать результат обратно — это отдельные операции и могут оказаться «бутылочным горлышком» в вашей
системе, график зависимости времени перекачки от размера данных приведен далее в статье.

Архитектуры, оптимизированные под высокие тактовые частоты, так же стали испытывать трудности, и не
только производственные. Производители чипов столкнулись с проблемами преодоления законов физики. Некоторые аналитики
даже предрекали, что закон Мура перестанет действовать. Но этого не произошло. Оригинальный смысл закона часто искажают,
однако он касается числа транзисторов на поверхности кремниевого ядра. Долгое время повышение числа транзисторов в CPU
сопровождалось соответствующим ростом производительности - что и привело к искажению смысла. Но затем ситуация
усложнилась. Разработчики архитектуры CPU подошли к закону сокращения прироста: число транзисторов, которое требовалось
добавить для нужного увеличения производительности, становилось всё большим, заводя в тупик.

Причина, по которой производителям GPU не столкнулись с этой проблемой очень простая: центральные процессоры
разрабатываются для получения максимальной производительности на потоке инструкций, которые обрабатывают разные данные (
как целые числа, так и числа с плавающей запятой), производят случайный доступ к памяти и т.д. До сих пор разработчики
пытаются обеспечить больший параллелизм инструкций - то есть выполнять как можно большее число инструкций параллельно.
Так, например, с Pentium появилось суперскалярное выполнение, когда при некоторых условиях можно было выполнять две
инструкции за такт. Pentium Pro получил внеочередное выполнение инструкций, позволившее оптимизировать работу
вычислительных блоков. Проблема заключается в том, что у параллельного выполнения последовательного потока инструкций
есть очевидные ограничения, поэтому слепое повышение числа вычислительных блоков не даёт выигрыша, поскольку большую
часть времени они всё равно будут простаивать.

Работа GPU относительно простая. Она заключается в принятии группы полигонов с одной стороны и генерации группы пикселей
с другой. Полигоны и пиксели независимы друг от друга, поэтому их можно обрабатывать параллельно. Таким образом, в GPU
можно выделить крупную часть кристалла на вычислительные блоки, которые, в отличие от CPU, будут реально использоваться.

Какие ограничения накладывает такая архитектура на выполняемые алгоритмы:

- Если мы выполняем расчет на GPU, то не можем выделить только одно ядро, выделен будет целый блок ядер (32 для NVIDIA).
- Все ядра выполняют одни и те же инструкции, но с разными данными, такие вычисления
  называются Single-Instruction-Multiple-Data или SIMD (хотя NVIDIA вводит свое уточнение).
- Из-за относительно простого набора логических блоков и общих регистров, GPU очень не любит ветвлений, да и в целом
  сложной логики в алгоритмах.

Какие возможности открывает:

- Собственно, ускорение тех самых SIMD-вычислений. Простейшим примером может служить поэлементное сложение матриц, его и
  давайте разберем.

Наиболее распространены две технологии, которые можно использовать для программирования под GPU:

- OpenCL
- CUDA

OpenCL – это стандарт, который поддерживают большинство производителей видеокарт, в т.ч. и на мобильных устройствах,
также код, написанный на OpenCL, можно запускать на CPU.

Использовать OpenCL можно из C/C++, есть биндинги к другим языкам.

CUDA – это проприетарная технология и SDK от компании NVIDIA. Писать можно на C/C++ или использовать биндинги к другим
языкам.

Сравнивать OpenCL и CUDA несколько не корректно, т.к. одно — стандарт, второе — целое SDK. Тем не менее многие выбирают
CUDA для разработки под видеокарты несмотря на то, что технология проприетарная, хоть и бесплатная и работает только на
картах NVIDIA. Тому есть несколько причин:

- Более продвинутое API
- Проще синтаксис и инициализация карты
- Подпрограмма, выполняемая на GPU, является частью исходных текстов основной (host) программы
- Собственный профайлер, в т.ч. и визуальный
- Большое количество готовых библиотек
- Более живое комьюнити

К особенностям стоит отнести то, что CUDA поставляется с собственным компилятором, который так же может скомпилировать
стандартный C/C++ код.

Есть большое количество библиотек, которые скрывают сложности разработки под GPU: XGBoost, cuBLAS,
TensorFlow, PyTorch и другие.

## Ограничение ресурсов

Как мы уже говорили, два основных ресурса видеокарты – это вычислительные ядра и память.

К примеру, у нас несколько процессов или контейнеров, использующих видеокарту, и хотелось бы иметь возможность поделить
видеокарту между ними. К сожалению, простого API для этого нет. NVIDIA предлагает технологию vGPU, но карту Tesla k80 я
не нашел в списке поддерживаемых, и насколько мне удалось понять из описания, технология больше заточена на виртуальные
дисплеи, чем на вычисления. Возможно, AMD предлагает что-то более подходящее.

Поэтому, если планируете использовать GPU в своих проектах, стоит рассчитывать на то, что приложение будет использовать
видеокарту монопольно, либо вы будете программно контролировать объем выделяемой памяти и количество ядер, используемых
для вычислений.

### Трансформация

У нас есть два массива, A и B, и мы хотим к каждому элементу массива A добавить элемент из массива B. Ниже приведен
пример на C, хотя, надеюсь, он будет понятен и тем, кто не владеет этим языком:

```c
void func(float *A, float *B, size) {
  for (int i = 0; i < size; i++) {
    A[i] += B[i]
  }
}
```

Классический обход элементов в цикле и линейное время выполнения.

А теперь посмотрим, как такой код будет выглядеть для GPU:

```c
void func(float *A, float *B, size) {
  int i = threadIdx.x;
  if (i < size)
    A[i] += B[i]
}
```

А вот здесь уже интересно, появилась переменная threadIdx, которую мы вроде бы нигде не объявляли. Да, нам предоставляет
ее система. Представьте, что в предыдущем примере массив состоит из трех элементов, и вы хотите его запустить в трех
параллельных потоках. Для этого вам бы понадобилось добавить еще один параметр – индекс или номер потока. Вот это и
делает за нас видеокарта, правда она передает индекс как статическую переменную и может работать сразу с несколькими
измерениями – x, y, z.

Еще один нюанс, если вы собираетесь запускать сразу большое количество параллельных потоков, то потоки придется разбить
на блоки (архитектурная особенность видеокарт). Максимальный размер блока зависит от видеокарты, а индекс элемента, для
которого выполняем вычисления, нужно будет получать так:

```c
int i = blockIdx.x * blockDim.x + threadIdx.x; // blockIdx – индекс блока, blockDim – размер блока, threadIdx – индекс
потока в блоке
```

В итоге, что мы имеем: множество параллельно работающих потоков, выполняющих один и тот же код, но с разными индексами, а
соответственно, и данными, т.е. тот самый SIMD.

Это простейший пример, но, если вы хотите работать с GPU, вашу задачу нужно привести к такому же виду. К сожалению, это
не всегда возможно и в некоторых случаях может стать темой докторской диссертации, тем не менее классические
алгоритмы все же можно привести к такому виду.

### Агрегация

Давайте теперь посмотрим, как будет выглядеть агрегация, приведенная к SIMD представлению:

У нас есть массив из n элементов. На первом этапе мы запускаем n/2 потоков и каждый поток складывает по два элемента,
т.е. за одну итерацию мы складываем между собой половину элементов в массиве. А дальше в цикле повторяем все то же самое
для вновь получившегося массива, пока не с агрегируем два последних элемента. Как видите, чем меньше размер массива, тем
меньше параллельных потоков мы можем запустить, т.е. на GPU имеет смысл агрегировать массивы достаточно большого
размера. Такой алгоритм можно применять для вычисления суммы элементов (кстати, не забывайте о возможном переполнении
типа данных, с которым вы работаете), поиска максимума, минимума или просто поиска.